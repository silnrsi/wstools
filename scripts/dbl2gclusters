#!/usr/bin/python3

import sys, os
from langtag import lookup, langtag
from iso639 import iso639_3_2


try:
    import dbl
except ImportError:
    sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'lib', 'wstools'))
    import dbl

try:
    import newdbl
except ImportError:
    relpath = os.path.join(os.path.dirname(__file__), '..', 'lib', 'wstools')
    sys.path.append(os.path.abspath(relpath))
    import newdbl

from sldr.ldml_exemplars import Exemplars
import argparse
import multiprocessing, logging

(skipfilesmap, knownvarsmap) = newdbl.exceptions()

allGeneratedFiles = {}


def process(infname, langCode, outdir=".", update=False):
    
    logging.debug("Processing: {}".format(infname))
    dblObj = dbl.DBL(infname)
    exemplars = Exemplars()
    exemplars.frequent = 0.0
    try:
        for t in dblObj.analyze_text():
            exemplars.process(t)
    except Exception as e:
        pass
    dblObj.close_project()
    
    knownVariant = False

    s = str(infname).rfind(langCode)
    dblfile = str(infname)[s:]

    if dblfile in skipfilesmap:
        reason = skipfilesmap[dblfile]
        logging.info("Skipping {}: {}".format(dblfile, reason))
        return

    if dblfile in knownvarsmap:
        langCode = knownvarsmap[dblfile]
        knownVariant = True
    
    a = str(langCode).rfind('-')
    if knownVariant and a > 0 and str(langCode)[a+1].isupper() and str(langCode)[a+2].isupper():
        ltag = langtag(langCode)
        #this is because some of the knownVariants distinguish between regions that langtags will consider one tag and will therefore remove the region under the "minimal" tag
    else:
        try:
            ltag = langtag(str(lookup(langCode).tag))
        except KeyError:
            iso639_3 = iso639_3_2(langCode)
            if iso639_3:
                ltag = langtag(str(lookup(iso639_3).tag))
                langCode = iso639_3
            else:
                ltag = langtag(langCode)
    r = str(ltag).rfind('-')
    if r > 0:
        lang = str(ltag)[:r]
    else: 
        lang = str(ltag) 
    if len(exemplars.script) > 0 and not knownVariant: 
        # for cases where the translation text is for a more specific locale than the "encompassed" one from the langCode, i.e. file might say "wsg" but the locale is actually "wsg_Telu"
        exemplarlangcode = lang + "-" + exemplars.script
        try:
            exemplartag = langtag(str(lookup(exemplarlangcode).tag))
        except KeyError: 
            exemplartag = langtag(exemplarlangcode)
        if exemplartag != ltag: 
            ltag = exemplartag
    
    # currently the missing method of getting an accurate langtag that the dbl import process uses but is absent here
    # is the method of looking at what the ldml file calls it
    # not sure how to deal with that yet

    path = os.path.join(outdir, str(ltag)[0])
    if not os.path.exists(path):
        os.makedirs(path)
    outfile = os.path.join(path, str(ltag).replace("-","_")+".tsv")
    if update and os.path.exists(outfile):
        logging.debug("Skipping: {}".format(infname))
        return False
    exemplars.analyze()
    clusters = {k.base+k.trailers: v for k, v in exemplars.raw_clusters.items()}
    if clusters == {}:
        logging.debug("Unable to get gclusters for {}".format(infname))

    allGeneratedFiles[dblfile] = outfile   # used to identify any duplicates that overrode each other

    with open(outfile, "w") as outf:
        outf.write("cluster\tcount\tcasing patterns\n")
        for k, v in sorted(clusters.items(), key=lambda a: (-a[1], a[0])):
            if k in exemplars.non_casing_chars.keys():
                # adds a column that notes if character only appears in a specific case in the DBL files
                outf.write("{}\t{}\t{}\n".format(k, v, exemplars.non_casing_chars[k]))
            elif k.upper() in exemplars.non_casing_chars.keys():
                # adds a column that notes if character only appears in a specific case in the DBL files
                outf.write("{}\t{}\t{}\n".format(k, v, exemplars.non_casing_chars[k.upper()]))
            else:
                outf.write("{}\t{}\t\n".format(k, v))
    logging.info("{} successfully processed under: {}".format(dblfile, outfile))
    return True

parser = argparse.ArgumentParser()
parser.add_argument('inputdir',help='Input directory containing project zips')
parser.add_argument('-o','--outdir',help='Output directory for generated files')
parser.add_argument('-l','--loglevel',help='Set logging level')
parser.add_argument('-L','--lang',help='Langtag to process')
parser.add_argument('-d','--dbl',action='store_true',help='Update DBL into inputdir')
parser.add_argument('-j','--jobs',type=int,help='number of parallel jobs, default 0 = num processors')
parser.add_argument('--listlangs',action='store_true',help='list all language codes available')
parser.add_argument('-u','--update',action='store_true',help='only pull new dbl files')
args = parser.parse_args()

if args.loglevel:
    logging.basicConfig(stream=sys.stdout, level=args.loglevel.upper(),
            format="%(levelname)s:%(module)s %(message)s")

if args.dbl:
    dreader = dbl.DBLReader()
    dreader.download(args.inputdir, lang=args.lang, update=args.update)

def doit(a):
    process(a[0], a[1], outdir=args.outdir, update=args.update)
    return True

if args.listlangs:
    dreader = dbl.DBLReader()
    entries = dreader.getEntries()
    for (entryId, entryInfo) in entries.items():
        (entryLangCode, entryAccessType) = entryInfo
        logging.debug("{} -> {}".format(entryLangCode, entryId))
else:
    filelist = [os.path.join(args.inputdir, f) for f in os.listdir(args.inputdir) if f.endswith(".zip")]
    allfiles = list(dbl.process_projects(filelist, args.lang))
    if args.jobs == 100:
        for a in allfiles:
            doit(a)
    else:
        p = multiprocessing.Pool(processes=args.jobs)
        list(p.map_async(doit, allfiles).get())
        
duplicates = {}
uniques = []
for g in allGeneratedFiles.keys():
    outputFile = allGeneratedFiles[g]
    if outputFile not in uniques:
        uniques.append(outputFile)
    else:
        for k in [k for k,v in allGeneratedFiles.items() if v == outputFile]:
            duplicates[k] = allGeneratedFiles[k]
if len(duplicates.items()) > 0: 
    print("Multiple DBL files saved to the same path, therefore overriding one or more of them. Still working on how to address this so be aware:")
    for d in duplicates.items():
        print("\t" + str(d))