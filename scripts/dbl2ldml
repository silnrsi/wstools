#!/usr/bin/python3
# coding: utf-8     # (reference: https://www.python.org/dev/peps/pep-0263/)

# Read a Paratext files (.lds and .ssf), convert the relevant data to LDML, and insert into an LDML file.
# Given the right options, can also download the Paratext files from the DBL.

import sys, traceback
import codecs, copy
import os, re
import logging
import multiprocessing
import json
from io import StringIO
from configparser import RawConfigParser
from xml.etree import ElementTree as etree
import xml.etree.cElementTree as et
from sldr import UnicodeSets, ducet
from icu import Script
from iso639 import iso639_3_2

# For calling DBLAuthV1 class:
# The requests module may need to be installed separately; it does not appear to be part of the standard Python.
from datetime import datetime
from time import mktime
from langtag import lookup, langtag
from sldr.ldml_exemplars import Exemplars, UCD
from sldr.ldml import Ldml, draftratings
from sldr.collation import Collation, CollElement

try:
    import newdbl
except ImportError:
    relpath = os.path.join(os.path.dirname(__file__), '..', 'lib', 'wstools')
    sys.path.append(os.path.abspath(relpath))
    import newdbl

silns = {'sil' : "urn://www.sil.org/ldml/0.1" }
gendraft = draftratings.get('generated', 5)

def gettext(e):
    return e.text if e is not None else ''

fontswap = {
    # list items in order are [font to replace it with, features, language setting, keep?]
    #"keep or not" refers to whether the font should still be listed without a link, or removed after being swapped out. True means keep it, False means delete it
    # If none is in the "font to replace it" spot, that means the font should just be deleted immediately. The code has a failsafe in case that deletes the only font. 
    #try to list these alphabetically for easy reference
    "Amdo Classic 1":["Noto Serif Tibetan", None, None, False],
    "Andika New Basic": ["Andika", None, None, True],
    "Andika SEB": ["Andika", "cv49=1 cv07=1", None, True],
    "Angsana New": ["Noto Sans Thai", None, None, False],
    "Annapurna": ["Annapurna SIL", None, None, False],
    "Annapurna SIL Beta 1 spvisarga2": ["Annapurna SIL", None, None, False],
    "Annapurna SIL Kham": ["Annapurna SIL", None, None, True],
    "Annapurna SIL Magar": ["Annapurna SIL", None, None, True],
    "Annapurna SIL Nepal": ["Annapurna SIL", "ss01=1 ss08=1 ss09=1 ss10=1", None, True],   # adds the features to adjacent Annapurna SIL files but also keeps the link to the specific nepal one 
    "Annapurna SIL Syuba": ["Annapurna SIL", None, None, True],
    "Apparatus SIL": ["Charis", None, None, False],
    "Arial" : ["Charis", None, None, False],
    "Arial Narrow" : ["Charis", None, None, False],
    "Arial Rounded MT Bold" : ["Charis", None, None, False],
    "Arial Unicode MS": ["Charis", None, None, False],
    "Book Antiqua": ["Charis", None, None, False],
    "Calibri" : ["Charis", None, None, False],
    "Californian FB": ["Charis", None, None, False],
    "Cambria": ["Charis", None, None, False],
    "Century": ["Charis", None, None, False],
    "Charis SIL": ["Charis", None, None, True],
    "Charis SIL Afr": ["Charis", "cv44=0 cv46=1 cv49=1 cv77=1 ss04=1", None, True],
    "Charis SIL AmArea": ["Charis", "ss04=1 cv43=2 cv68=1 cv77=1 cv90=1", None, True],
    "Charis SIL Charis SIL Literacy": ["Charis", "ss01=1", None, False],
    "Charis SIL Compact": ["Charis", None, None, True],      #typetuned to have Tight spacing
    "Charis SIL Ghana": ["Charis", "cv43=0 cv46=1", None, True],
    "Charis SIL Lit": ["Charis", "ss01=1", None, True],
    "Charis SIL Literacy": ["Charis", "ss01=1", None, True],
    "Charis SIL Mali": ["Charis", "cv43=0 cv44=0 cv46=1", None, True],
    "Comic Sans MS": ["Charis", None, None, False],
    "Constantia": ["Charis", None, None, False],
    "Courier New": ["Charis", None, None, False],
    "Cyrillic SILCharis": ["Charis", None, None, False],
    "Daasun~1 Daasun~1 SILCharis": ["Charis", None, None, False],
    "David CLM": ["Ezra SIL", None, None, False],
    "David Libre": ["Ezra SIL", None, None, False],
    "DejaVu Serif": [None, None, None, False],
    "Garamond": ["Charis", None, None, False],
    "Gautami": ["Noto Sans Telugu", None, None, False],
    "Gentium Plus": ["Gentium", None, None, False],
    "Gentium Basic": ["Gentium", None, None, False],
    "Georgia": ["Charis", None, None, False],
    "Khmer Barkaew": ["Khmer Busra", None, None, False],
    "Khmer Dakdam": ["Khmer Busra", None, None, False],
    "Khmer Dakdam YY": ["Khmer Busra", None, None, False],
    "Khmer Dakdam1": ["Khmer Busra", None, None, False],
    "Khmer Leak Laom": ["Khmer Mondulkiri", None, None, False],
    "Khmer OS": ["Khmer Mondulkiri", None, None, False],
    "Khmer Oyadaw": ["Khmer Mondulkiri", None, None, False],
    "Khmer Yeaklom": ["Khmer Mondulkiri", None, None, False],
    "Kristen ITC":["Charis", None, None, False],
    "Kruti Dev 016":["Annapurna SIL", None, None, False],
    "Lateef Haz Kas Low": ["Lateef","cv62=1", None, False],
    "Lateef Hazaragi": ["Lateef","cv62=1", None, False],
    "Lato": ["Charis", None, None, False],
    "Maiandra GD": ["Charis", None, None, False],
    "Mangal":  ["Annapurna SIL", None, None, False],
    "Microsoft Himalaya":["Noto Serif Tibetan", None, None, False],
    "Monlam Uni OuChan2": ["Noto Serif Tibetan", None, None, False],
    "MS Outlook": ["Charis", None, None, False],
    "Rachana": ["Noto Sans Malayalam", None, None, False],
    "Rachana Edited": ["Noto Sans Malayalam", None, None, False],
    "Sakkal Majalla": ["Scheherazade New", None, None, False],
    "Scheherazade": ["Scheherazade New", None, None, False],
    "Scheherazade KasLow": ["Scheherazade New", "cv62=1", None, True],
    "Scheherazade M4z": ["Scheherazade New", None, None, False],
    "Scheherazade Urdu": ["Scheherazade New", None, "ur", True],
    "ScheherazadeShina32 Shina32": [None, None, None, False],
    "SIL Ezra": ["Ezra SIL", None, None, False],
    "SIL Galatia": ["Galatia SIL", None, None, False],
    "SILCharis": ["Charis", None, None, False],
    "SILDoulos IPA93": ["Doulos SIL", "cv43=2, cv77=1, ss05=1", None, False],
    "Sylfaen": ["Charis", None, None, False],
    "Tahoma": ["Charis", None, None, False],
    "Tibetan Machine Uni":["Noto Serif Tibetan", None, None, False],
    "Times New Roman" : ["Charis", None, None, False],
    "Verdana": ["Charis", None, None, False],
    "XenoType TB Drepung":["Noto Serif Tibetan", None, None, False]
}

fontExceptions = {
    # for cases where there's no way to hard-code this change, but you don't want to double check to make sure it didn't replace it every time you merge in new data
    "emp_3c8ed09b9e38c6f3.zip" : "Charis",
    "ndz_b5472a584e2d47b2.zip": "Charis"
}

stowaways = {
    # fonts that appear in files where they should not based on the script. update as needed. 
    "Charis" : ["Latn", "Cyrl"],
    "Doulos SIL" : ["Latn", "Cyrl", "Grek", "Copt"],
    "Annapurna SIL": ["Deva"],
    "Annapurna SIL Nepal": ["Deva"],
    "Noto Sans Thai": ["Thai"],
    "Noto Sans Malayalam": ["Mlym"],
    "Tagmukay": ["Tfng"],
    "Abyssinica SIL": ["Ethi"]
}

dontaddcollation = [
    #files that have currently empty collation but DO NOT want the collation from the dbl file added to it
    #please put in alphabetical order for easier reference, and a comment explaining why this collation should be excluded. 
    "aeu",  # standard a-z collation except for apostrophe after z, but word-forming apostrophe doesn't appear in translation text
    "aia",  # standard a-z collation except for ÃŸ before a, which doesn't appear in the orthography at all
    "cco",  # spanish-based a-z collation that doesn't represent the actual orthography
    "chq",  # spanish-based a-z collation that doesn't represent the actual orthography
    "chz",  # spanish-based a-z collation that doesn't represent the actual orthography
    "cso",  # spanish-based a-z collation that doesn't represent the actual orthography
    "ctp",  # spanish-based a-z collation that doesn't represent the actual orthography
    "djk",  # spanish-based a-z collation that doesn't represent the actual orthography
    "dwz",  # standard a-z collation
    "emp",  # something in the simple collation is fundamentally broken when it tries to process it, and the values in it don't reflect the orthography anyway
    "hbb",  # sort order makes no sense, could be legit, but hard to say 
    "kmk",  # standard a-z collation
    "kyg",  # standard a-z collation
    "msk",  # standard a-z collation
    "nsn",  # standard a-z collation
    "sxb",  # something in the simple collation is fundamentally broken when it tries to process it, and the values in it don't reflect the orthography anyway
    "zad",  # something in the simple collation is fundamentally broken when it tries to process it, and the values in it don't reflect the orthography anyway
]

newDblFiles = {}
allGeneratedFiles = {}
runFiles = []
fontIssues = {}

class Puamap:

    start = 0xF134
    mapstr = "âŒŠâŒ‹                                                          á·‚á·„á·…á·†á·‡á·ˆÍœÍžá·Šá·‰Ìˆá·½    áµá¶®áµ‘áµƒáµ„áµ…áµ‡áµˆáµ‰áµŠáµ‹á¶Ÿáµáµáµ’áµ“áµ–áµ—áµ˜áµšáµ›Ë‹ËˆËŠêœ—êœ˜êœ™êœšêœ›êœœêœêœžá¶›ðžƒá¶ðžŽðžá¶ á¶¢á¶¤á¶¦á¶¡á¶©ðž¢á¶±êŸ¹ðž£á¶´á¶¶á¶·á¶­á¶ºðž²ðž‘á¶»á¶½á¶¾á¶œá¶žá¶£ðž•á¶¨á¶ªá¶«á¶¬á¶¯á¶°á¶²á¶³á¶µá¶¹á¶¼Ë€á¶¿á¶¥á¶§á¶¸ ðž  êœ’êœ“êœ”êœ•êœ–êœˆêœ‰êœŠêœ‹êœŒêœêœŽêœêœêœ‘êœ€êœ‚êœ„êœ†êœêœƒêœ…êœ‡êžˆË¬êž‰êžŠ                     á´€È¡á´‡È´ÈµÈ¶Ê®Ê¯â±­â±°ÉƒÈ¼á¶‘ â±¡â± áµ½É‹ÉŠÉÉŒâ±¤á¶˜Æ·É„É…â±³â±²á¶šêžŒÉ‚È½É‡É†â±¨â±§á¶€êž”á¶á¶‚á¶ƒá¶„á¶…á¶†á¶‡á¶ˆá¶‰á¶Šá¶‹á¶Œá¶á¶ŽêŸ†ð¼˜á¶á¶á¶’á¶“á¶”á¶•á¶–á¶—á¶™áµ¾È¸È¹â±¢ÉÉŽêœ«êœ­â“â’¶áµ¬áµ­áµ®áµ¯áµ°áµ±áµ²áµ³áµ´áµµáµ¶áµ»áµ¿È¿É€  êœ§â±®â±£áµ¼â±´â±±É‰Éˆâ±ªâ±©â±¬â±«êžŽð¼†ð¼„ð¼ˆêž‹êžêž²                                                                                                                                                   ×†×…                              Ó¶Ó·Ô’Ô“Ó¼Ó½  Ó¾Ó¿ÔÔ‘Ô¦"
    def __init__(self):
        ranges = []
        s = None
        for i, c in enumerate(self.mapstr):
            if c == " ":
                if s is not None:
                    ranges.append([s, i])
                s = None
            elif s is None:
                s = i
        self.re = re.compile("["+"".join("{}-{}".format(chr(self.start+r[0]), chr(self.start+r[1])) for r in ranges) + "]")
        self.ucd = UCD()

    def __call__(self, s):
        return self.ucd.normalize('NFC', self.re.sub(lambda m:self.mapstr[ord(m.group(0)) - self.start], s))

puamap = Puamap()


ssfmap = {
    # LanguageIsoCode is passed in
    'DefaultFont': ('special/sil:external-resources/sil:font[@types="default"]/@name', ''),
    'DefaultFontSize': ('special/sil:external-resources/sil:font[@types="default"]/@size', '0'),
    # ValidCharacters is unused
    'Pairs': ('delimiters/special/sil:matched-pairs', lambda x: " ".join("{}/{}".format(a.get('open',''), a.get('close','')) for a in x)),
    'Quotes': ('delimiters', lambda x: " ".join(gettext(x.find('quotation'+a)) for a in ('Start', 'End'))),
    'InnerQuotes': ('delimiters', lambda x: " ".join(gettext(x.find('alternateQuotation'+a)) for a in ('Start', 'End'))),
    'InnerInnerQuotes': ('delimiters/special/sil:quotation-marks/sil:quotation[@level="3"]', lambda x: " ".join(x.get(a) or '' for a in ('open', 'close'))),
    'ContinueQuotes': ('delimiters/special/sil:quotation-marks/sil:quotation[@level="1"]/@continue', ''),
    'ContinueInnerQuotes': ('delimiters/special/sil:quotation-marks/sil:quotation[@level="2"]/@continue', '')
    # nothing else is used
    }

class Ssf:
    def __init__(self):
        pass

    def parseSsf(self, ssfFilename):
        if ssfFilename is None:
            return
        if isinstance(ssfFilename, str):  # Python 2 uses basestring
            ssfFile = open(ssfFilename, 'rb')
        else:
            ssfFile = ssfFilename

        self.ssfLangData = {  # initializing everything to None simplifies the processing
            'LanguageIsoCode': None,
            'DefaultFont': None,
            'DefaultFontSize': None,
            'ValidCharacters': None,
            'Pairs': None,
            'Quotes': None,
            'InnerQuotes': None,
            'InnerInnerQuotes': None,
            'ContinueQuotes': None,
            'ContinueInnerQuotes': None,
            'Continuer': None,
            'InnerContinuer': None,
            'InnerInnerContinuer': None,
            'VerboseQuotes': None,
            'ValidPunctuation': None}
        
        for (event, e) in etree.iterparse(ssfFile, events=('start', 'end')) :
            ##print(event + ": " + e.tag)
            if event == 'start' :
                if e.tag in ssfLangData :
                    self.ssfLangData[e.tag] = e.text
        s = self.ssfLangData.get('LanguageIsoCode', '').rstrip(':')
        self.ssfLangData['LanguageIsoCode'] = s.replace(':', '-')

    def fromLdml(self, ldml, langcode):
        self.ssfLangData = {'LanguageIsoCode': langcode}
        for k, v in ssfmap.items():
            e = ldml.find(v[0])
            if e is None and isinstance(v[1], str):
                res = v[1]
            elif e is not None:
                res = v[1](e)
            else:
                res = ''
            self.ssfLangData[k] = res

    def get_lang(self):
        return self.ssfLangData['LanguageIsoCode']

    def process(self, ldml, filename):
        if self.ssfLangData['DefaultFont'] or self.ssfLangData['DefaultFontSize'] :
            _addLdsSsfDataFont(ldml, self.ssfLangData['DefaultFont'], float(self.ssfLangData['DefaultFontSize']) * 14, filename)
        if self.ssfLangData['Pairs'] :
            self._addSsfDataPairs(ldml, self.ssfLangData['Pairs'])
        if self.ssfLangData['Quotes'] :
            self._addSsfDataQuotes(ldml, self.ssfLangData['Quotes'])
        if self.ssfLangData['InnerQuotes'] :
            self._addSsfDataInnerQuotes(ldml, self.ssfLangData['InnerQuotes'])
        if self.ssfLangData['InnerInnerQuotes'] :
            self._addSsfDataInnerInnerQuotes(ldml, self.ssfLangData['InnerInnerQuotes'])

    def _addSsfDataPairs(self, ldml, pairsValue) :
        # Pairs ->
        # delimiters/special/sil:matched-pairs/sil:matched-pair/@open, @close
        pairs = set(tuple(x.split('/')) for x in pairsValue.split(' '))
        if not len(pairs):
            return
        matchedElem = ldml.ensure_path('delimiters/special/sil:matched-pairs')[0]
        if ldml.get_draft(matchedElem) < gendraft and len(matchedElem) > 0:
            return
        for e in list(matchedElem):
            t = (e.get('open', ''), e.get('close', ''))
            if t not in pairs:
                matchedElem.remove(e)
            else:
                pairs.remove(t)
        for p in sorted(pairs):
            ldml.addnode(matchedElem, 'sil:matched-pair', open=p[0], close=p[1])

    def _addSsfDataQuotes(self, ldml, quotesValue) :
        # Quotes ->
        # delimiters/quotationStart, delimiters/quotationEnd
        (qStart, qEnd) = quotesValue.split(' ')
        if not qStart and not qEnd:
            return
        qStartElem = ldml.ensure_path('delimiters/quotationStart', text=qStart)
        qEndElem = ldml.ensure_path('delimiters/quotationEnd', text=qEnd)

    def _addSsfDataInnerQuotes(self, ldml, quotesValue) :
        # InnerQuotes ->
        # delimiters/alternateQuotationStart, delimiters/alternateQuotationEnd
        (qStart, qEnd) = quotesValue.split(' ')
        if not qStart and not qEnd:
            return
        qStartElem = ldml.ensure_path('delimiters/alternateQuotationStart', text=qStart)
        qEndElem = ldml.ensure_path('delimiters/alternateQuotationEnd', text=qEnd)

    def _addSsfDataInnerInnerQuotes(self, ldml, quotesValue) :
        # InnerInnerQuotes ->
        # delimiters/special/sil:quotation-marks[@level="3"]/@open, @close
        (qStart, qEnd) = quotesValue.split(' ')
        if not qStart and not qEnd:
            return
        qMark3Elem = ldml.ensure_path('delimiters/special/sil:quotation-marks/'
                    'sil:quotation[@level="3"][@open="{}"][@close="{}"]'.format(qStart, qEnd))[0]

    def _addSsfDataContinueQuotes(self, ldml, contValue, contInnerValue) :
        # ContinueQuotes ->
        # delimiters/special/sil:quotation-marks[@paraContinueType]/@open, @close
        if _valueNotNo(contValue):
            ldml.ensure_path('delimiters/special/sil:quotation-marks/sil:quotation[@level="1"][@continue="{}"]'.format(contValue))
        if _valueNotNo(contInnerValue):
            ldml.ensure_path('delimiters/special/sil:quotation-marks/sil:quotation[@level="2"][@continue="{}"]'.format(contInnerValue))


def _valueNotNo(value) :
    if value is None :
        return False
    if value.lower() == "no" :
        return False
    if value.lower() == "false" :
        return False
    return True

def processLds(ldml, ldsFile, ducetDict, replaceColl, filename, script):
    if ldsFile.peek(3)[:3] == b"\xef\xbb\xbf":
        ldsFile.read(1)
    ldsConfig = RawConfigParser()
    try:
        ldsConfig.read_file(ldsFile)
    except:
        logging.debug("Unable to read lds file in {}".format(filename))
        return (None, None)

    if ldsConfig.has_section('General'):
        fontValue = None
        sizeValue = None
        if ldsConfig.has_option('General', 'font'):
            fontValue = ldsConfig.get('General', 'font')
        if ldsConfig.has_option('General', 'size'):
            sizeValue = ldsConfig.get('General', 'size')
        
        _addLdsSsfDataFont(ldml, fontValue, float(sizeValue or 0), filename, script)

    if replaceColl:
        simpleColls = readLdsCollation(ldsConfig)
        (sortColl, alphabet) = calcSortLdsData(ldml, filename, simpleColls, ducetDict, script)
        return (sortColl, alphabet)
    return (None, None)


def _addLdsSsfDataFont(ldml, defaultFontValue, defaultSizeValue, filename, script=None) :
    # DefaultFont, DefaultFontSize ->
    # special/sil:external-resources/sil:fontrole[@types="default"]/sil:font[@name, @size]
    # TODO: this isn't quite right since it should handle an existing sil:fontrole[@types="default heading"]
    #fontElemp = ldml.ensure_path('special/sil:external-resources/sil:fontrole[@types="default"]')[0]
    # Or (more likely) this might come from the .lds file.

    #ideally (?) have both the charis one with features, and the typetuned one for clarity.
    subsetFontValue = None
    features = None

    if script == None:
        s = ldml.find('identity/script')
        i = ldml.root.find(".//identity/special/sil:identity", {v:k for k,v in ldml.namespaces.items()})
        if s is not None:
            script = s.text
        elif i is not None:
            script = i.get("script")
    
    if script is not None:
        if defaultFontValue in stowaways.keys() and script not in stowaways[defaultFontValue]:
            #if the font is not for the correct script, bail
            return

    if defaultFontValue in fontswap.keys():
        subsetFontValue = defaultFontValue
        defaultFontValue = fontswap[defaultFontValue][0]
    a = str(filename).rfind('/')
    if a > 0:
        filename = str(filename)[a+1:]
    if filename in fontExceptions.keys():
        defaultFontValue = fontExceptions[filename]
    xpath = 'special/sil:external-resources/sil:font[@types="default"][@name="{}"]'.format(defaultFontValue)
    # if features is not None:
    #     xpath += '[@features="{}"]'.format(features)
    allfonts = set([e.get('name') for e in ldml.findall('special/sil:external-resources/sil:font')])
    if defaultFontValue in allfonts:
        return

    defaultSizeFactor = 1.0
    if defaultSizeValue:
        # We treat Times New Roman size 14 as the standard.
        defaultSizeFactor = int(defaultSizeValue) / 14
        # Round to the nearest .05:
        defaultSizeFactor = (int(defaultSizeFactor * 20)) / 20
        # Factors close to 1.0 are not really significant.
        if defaultSizeFactor > .92 and defaultSizeFactor < 1.08:
            defaultSizeFactor = 1.0
    exResNode = ldml.ensure_path('special/sil:external-resources')[0]
    fontElemNode = None
    for e in ldml.findall('special/sil:external-resources/sil:font'):
        if 'default' in e.get("types", ""):
            fontElemNode = e
            if (defaultFontValue == "" or e.get('name') == defaultFontValue) \
                    and (defaultSizeValue < 0.001 or float(e.get('size', '1.0')) == defaultSizeFactor):
                return

    if defaultSizeFactor != 1.0:
        xpath += '[@size="{}"]'.format(defaultSizeFactor)

    if fontElemNode is None:
        if defaultFontValue == "":
            return
        fontElemGen = ldml.ensure_path(xpath, draft="generated")[0]
    else:
        fontElemGen = None
        fontNamePrev = fontElemNode.get('name', "")
        fontSizePrev = fontElemNode.get('size', None)

        if fontNamePrev == "":
            # Throw away old empty value.
            if defaultFontValue is None or defaultFontValue == '':
                # Remove altogether.
                if fontElemNode is not None:
                    exResNode.remove(fontElemNode)
            else:
                fontElemNode = ldml.ensure_path(xpath, draft="generated")[0]
        elif fontNamePrev != defaultFontValue:
            # mismatch; old value is marked suspect and alt=proposed-dbl, new value is draft
            fontElemSuspect = fontElemNode    #ldml.ensure_path(xpath)[0]
            for alt in getattr(fontElemNode, 'alternates', {}).values():
                if alt.get('name', '') == defaultFontValue and alt.get('size', '1.0') == str(defaultSizeFactor):
                    break
            else:
                fontElemSuspect.parent.remove(fontElemSuspect)
            fontElemNode = ldml.ensure_path(xpath, draft="generated", matchdraft="draft")[0]
        else:
            # Font values match.
            if fontSizePrev is not None and defaultSizeFactor == 1.0:
                del fontElemNode.attrib['size']
            elif defaultSizeFactor != 1.0:
                fontElemNode.set('size', str(defaultSizeFactor))

def processMetadata(ldml, fname):
    etree = et.parse(fname)
    langNode = etree.getroot().find('language')
    if langNode is None:
        return
    ltagnameNode = langNode.find('ldml')
    if ltagnameNode is None:
        ltagnameNode = ldml.find('identity/language')
        ltagname = ltagnameNode.get("type", "")
    else:
        ltagname = ltagnameNode.text or ""
    stagnameNode = langNode.find('scriptCode')
    localnameNode = langNode.find('nameLocal')
    if localnameNode is not None and ltagnameNode is not None:
        ltag = langtag(ltagname)
        ldml.ensure_path('localeDisplayNames/languages/language[@type="{}"]'.format(ltag.lang),
                text=localnameNode.text)
    engnameNode = langNode.find('name')
    if engnameNode is not None or ldml.get_draft(engnameNode) >= gendraft:
        ldml.ensure_path('localeDisplayNames/special/sil:names/sil:name[@xml:lang="en"]',
                text=engnameNode.text)
    res = ltagname + ("-"+stagnameNode.text if stagnameNode else "")
    return res

def readLdsCollation(ldsConfig):
    valueList = []
    # Read sorted characters lists from .lds file.
    if ldsConfig.has_section('Characters') :
        cntr = 1
        keepGoing = True
        while keepGoing :  ### and cntr < 100:
            strCntr = str(cntr)
            if len(strCntr) < 2 : strCntr = '0' + strCntr
            key = 'Chr' + strCntr
            if not ldsConfig.has_option('Characters', key) :
                keepGoing = False
            else :
                value = ldsConfig.get('Characters', key)
                if value == "'/'":
                    value = "'"  # kludge; why does the get() function return the wrong value?
                valueList.append(value)
            cntr = cntr + 1
    if valueList is not None:
        puaValueList = [puamap(s) for s in valueList]
    return puaValueList

def calcSortLdsData(ldml, filename, valueList, ducetDict, script) :
    # Generate a data structure similar to a sort tailoring for the list of characters.
    sortResult = []
    collObj = Collation(ducetDict)
    alphabet = []
    simple = True
    simplelist = list("abcdefghijklmnopqrstuvwxyz'")
    charError = False
    monocameral = ['ÃŸ'] # characters that only have one case in most files, and shouldn't be capitalized in collation. this may be removed in favor of (ew) manual review if nothing is universal
    stowaways = ['\u200c']   # add to this list things that might sneak into stuff unexpectedly. Note that for U+200c this only works because if it was intentional, it'd be between two characters, and this section only allows for sets of 2
    #print(valueList)
    cleanedValueList = []
    for v in valueList:
        v2=v.replace("#","/")
        v = v2
        cleanedValueList.append(v)
    valueList = cleanedValueList
    # print(cleanedValueList)
    # print(cleanedValueList)
    # print(valueList)
    # print(script)
    # for value in valueList:
    #     newValue = ""
    #     for v in value:
    #         if Script.getShortName(Script.getScript(v)) != script and Script.getShortName(Script.getScript(v)) not in ['Zyyy', 'Zinh']:
    #         # an attempt to clean up some REALLY wonky collations, specifically removing items that aren't in the proper script for the file
    #             continue
    #         else:
    #             newValue += v
    #     noSlash = re.sub("/", "", newValue)
    #     diacriticCount = 0
    #     for s in noSlash:
    #         if Script.getShortName(Script.getScript(s)) == "Zinh":
    #             diacriticCount += 1
    #     if diacriticCount != 0 and (diacriticCount + 1) == len(noSlash):
    #         noSlash = ""
    #     if len(newValue) and not newValue.isspace() and not noSlash.isspace() and len(noSlash) > 0:
    #         cleanedValueList.append(newValue)
    # valueList = cleanedValueList
    # print(valueList)
    # if len(cleanedValueList) > 0:
    #     collObj.convertSimple(valueList)
    # else: 
    #     collObj = None
    collObj.convertSimple(valueList)
    # print(collObj)
    # if len(valueList) > 0 :
    #     currBase = None
    #     for value in valueList :
    #         uValue = value  # Python 2: .decode('utf-8')
    #         spaceItems = uValue.split(' ')
    #         if len(spaceItems) == 2 and spaceItems[0].lower() == spaceItems[1].lower():
    #             # Kludge: deal with a limitation of Paratext. Since these items are case equivalent, the user probably
    #             # intended x/X rather than x X and was not permitted by Paratext.
    #             value = value.replace(' ', '/')
    #             uValue = value  # Python 2: .decode('utf-8')
    #             spaceItems = uValue.split(' ')
    #         sortSpecItems = []
    #         spaceSep = "&"
    #         prevSlashItems = None
    #         currLevel = 1
    #         for spaceItem in spaceItems :
    #             if spaceItem != '/':
    #                 slashItems = spaceItem.split('/')
    #             else:
    #                 slashItems = [spaceItem]
    #             # Kludge to handle something like xX which should really be x/X
    #             if len(slashItems) == 1 and len(slashItems[0]) == 2 :
    #                 c1 = (slashItems[0])[0:1]
    #                 c2 = (slashItems[0])[1:]
    #                 if c1 in stowaways:   
    #                     slashItems = [c2]
    #                 elif c2 in stowaways:
    #                     slashItems = [c1]
    #                 elif ducet.ducetCompare(ducetDict, c1, c2) == 3 : # case equivalent with x <<< X
    #                     # Assume a typo where they left out the slash.
    #                     slashItems = [c1, c2]
    #             if len (slashItems) == 2 and len(slashItems[0]) > 1 and slashItems[0][1].lower() != slashItems[0][1].upper():
    #                 #for not-fully-expanded digraphs (doesn't handle multigraphs atm). Final "and" prevents this from catching decomposed diacritics
    #                 slashItems = [slashItems[0].lower()]
    #             if len(slashItems) == 1 and len(slashItems[0]) < 10 and slashItems[0] not in monocameral:        
    #                 # previously, the third requirement said 'and len(slashItems[0]) > 1' but was changed to a list of exceptions since it also skipped other characters that were left uncapitalized incorrectly in the simple collation
    #                 # specifically the exceptions should hold anything that shouldn't be expanded out to have its upper and lowercase combinations
    #                 s = slashItems[0]
    #                 slashSet = set()
    #                 if sum(1 if c.lower() == c else 0 for c in s) == len(s):
    #                     for i in range(2 ** len(s)):
    #                         v = "".join(c.upper() if (i & (1 << j)) != 0 else c for j,c in enumerate(s))
    #                         if len(v) > 1 and v[0].islower() and any(b.isupper() for b in v[0:]):
    #                             # cuts out the "lopsided" case combinations that unicode standard deems unnecessary (i.e. "nY", "nGy", "nGY", "ngY", etc.)
    #                             #simply remove this 'if' statement and the continue if we decide later we want them in there after all
    #                             continue 
    #                         slashSet.add(v)
    #                     slashSet.remove(s)
    #                     slashItems.extend(slashSet)
    #             if len(slashItems[0]) > 0:
    #                 if Script.getShortName(Script.getScript(slashItems[0][0])) != script and Script.getShortName(Script.getScript(slashItems[0][0])) != 'Zyyy':
    #                 # an attempt to clean up some REALLY wonky collations, specifically removing items that aren't in the proper script for the file
    #                     continue
    #             try:
    #                 slashItems.sort(key=ducet.keyfn(ducetDict, 3))
    #                 slashItems.reverse()
    #             except TypeError:
    #                 charError = True
    #                 pass
    #             if simple:
    #                 if not len(simplelist) or slashItems[0] != simplelist.pop(0):
    #                     simple = False
    #             for s in slashItems:
    #                 if alphabet == []:
    #                     firstChar = s
    #                 if s != "a":
    #                     before = 0
    #                     if currBase is not None:
    #                         try:
    #                             collObj[s] = CollElement(currBase, currLevel, before)    
    #                         except KeyError as errorname:
    #                             #logging.debug("Abnormalities in simple collation in {} ".format(filename) + str(type(errorname).__name__) + " - " + str(errorname))
    #                             continue
    #                 if s == "a":
    #                     before = 1
    #                     if currBase is not None:
    #                         try: 
    #                             collObj[firstChar] = CollElement(s, currLevel, before)
    #                         except KeyError as errorname:
    #                             #logging.debug("Abnormalities in simple collation in {} ".format(filename) + str(type(errorname).__name__) + " - " + str(errorname))
    #                             continue
    #                 currLevel = 3
    #                 currBase = s
    #                 alphabet.append(s)
    #             currLevel = 2
    # print(collObj.asICU())
    simple = False  #shortcut until new way to tell if simple is devised
    # print(alphabet)
    # print(collObj)
    if charError:
        logging.debug("Unrecognized character(s) in simple collation for {}; recommend manually confirming collation is correct".format(filename))
    return (collObj, alphabet) if not simple else (None, None)

def cleanUpOldSortSpec(oldValue):
    newValue = oldValue.replace('\/', '/')
    return newValue

def _debugStr(item):
    if isinstance(item, int):
        item = [item]
    result = "".join(map(unichr, item))
    result += " =" + " ".join(map(hex, item))
    return repr(result)

def addMissingFontData(ldml, fonts, script):
    
    fontMissing = False
    urlMissing = False
    linklessFonts = []

    lffUrls = {
        "Charis" : "charis",
        "Aboriginal Sans" : "aboriginalsans",
        "Abyssinica SIL" : "abyssinicasil",
        "Akatab" : "akatab",
        "Alkalami" : "alkalami",
        "Andika" : "andika",
        "Annapurna SIL" : "annapurnasil",
        "Annapurna SIL Nepal" : "annapurnasilnepal",
        "Awami Nastaliq" : "awaminastaliq",
        "BJCree UNI" : "bjcree2uni",
        "BJCree2 UNI" : "bjcreeuni",
        "Dai Banna SIL" : "daibannasil",
        "Doulos SIL" : "doulossil",
        "Dukor" : "dukor",
        "Ezra SIL" : "ezrasil",
        "Galatia SIL" : "galatiasil",
        "Gentium" : "gentium",
        "Harmattan" : "harmattan",
        "Badami" : "badami",
        "Lateef" : "lateef",
        "Lisu Bosa" : "lisubosa",
        "Mingzat" : "mingzat",
        "Khmer Mondulkiri" : "khmermondulkiri",
        "Namdhinggo" : "namdhinggosil",
        "Nokyung" : "nokyung",
        "Noto Sans Mongolian" : "notosansmongolian",
        "Noto Sans Sundanese" : "notosanssundanese",
        "Nuosu SIL" : "nuosusil",
        "Padauk" : "padauk",
        "Payap Lanna" : "payaplanna",
        "Scheherazade New" : "scheherazadenew",
        "Sapushan" : "sapushan",
        "Shimenkan" : "shimenkan",
        "Sophia Nubian" : "sophianubian",
        "Tai Heritage Pro" : "taiheritagepro",
        "ThiruValluvarCTT" : "thiruvalluvar",
        "Khmer Busra": "khmerbusra",
        "Kay Pho Du": "kayphodu",
        "Surma": "surma",
        "Tagmukay": "tagmukay",
        "Japa Sans Oriya": "japasansoriya",
        "Sarabun":"sarabun",
        "Saysettha":"saysettha",
        "Jomolhari":"jomolhari"
    }

    notLffUrls = {
        #note: these links seem to lead to a read-only version of the noto fonts repo. in future, may shift so that these instead direct to lff versions of these links
        "Noto Sans Telugu": "https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSansTelugu/NotoSansTelugu-Regular.ttf",
        "Noto Sans": "https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSans/NotoSans-Regular.ttf",
        "Noto Serif Tibetan": "https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSerifTibetan/NotoSerifTibetan-Regular.ttf",
        "Noto Sans Thai": "https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSansThai/NotoSansThai-Regular.ttf",
        "Noto Sans Malayalam": "https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSansMalayalam/NotoSansMalayalam-Regular.ttf",
        "Noto Sans Syloti Nagri": "https://github.com/notofonts/noto-fonts/raw/main/hinted/ttf/NotoSansSylotiNagri/NotoSansSylotiNagri-Regular.ttf",
        "Noto Sans Kannada": "https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSansKannada/NotoSansKannada-Regular.ttf",
        "Noto Serif Kannada": "https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSerifKannada/NotoSerifKannada-Regular.ttf"
    }

    automaticDefaults = [
        # fonts that NEED the default tag to ensure it pops up first in lff
        # example: Annapurna SIL Nepal often shows up next to Annapurna SIL, but the former is more specific than the latter
        #this isn't perfect: for example if Annapurna is already default then Annapurna SIL Nepal will be second alphabetically and therefore lff will still show default Annapurna
        # a more complex bit of code might need to be written using a dictionary saying "if both value and key are in this font list remove default from one and add default to the other"
        "Annapurna SIL Nepal",
        "Badami"
    ]

    defaultFonts = {
        #this could probably be made to automatically pull from script2font.csv but for now it's manual
        "Latn": "Charis",
        "Deva": "Annapurna SIL"
    }

    def _handleNoFonts(fonts):
        # for when there are no fonts at all
        if len(fonts) == 0:
            # if new file has no fonts in it at all
            # code this to actually interact with script to font or something later, but for now...
            if script in defaultFonts.keys():
                newFont = ldml.ensure_path('special/sil:external-resources/sil:font[@types="default"][@name="{}"]'.format(defaultFonts[script]))[0]
                fonts.append(newFont)
        return fonts        
    
    def _addLink(font, fontElem):
        urlEl = ldml.addnode(fontElem, 'sil:url', returnnew=True)
        if font in lffUrls.keys():
            urlEl.text = "https://lff.api.languagetechnology.org/family/" + lffUrls[font]
            return False
        elif font in notLffUrls.keys():
            urlEl.text = notLffUrls[font]
            return False
        else:
            fontElem.remove(urlEl)
            if font not in linklessFonts:
                linklessFonts.append(font)
            return True

    fonts = _handleNoFonts(fonts)

    fontList = {}
    newSwaps = {}
    for fontElement in fonts:
        name = fontElement.get("name")
        fontList[name]=fontElement

    for font, fontElem  in fontList.items():
        originalFont = None
        swapperFont = None
        swapNeeded = False
        features = None
        langSetting = None
        keep = True
        linksMissing = False
        if font in fontswap.keys(): 
            originalFont = font
            swapperFont= fontswap[font][0]
            swapNeeded = True
            features = fontswap[font][1] 
            langSetting = fontswap[font][2] 
            keep = fontswap[font][3]
        if font == swapperFont:
            #for cases where the font is the same, it just needs new features. idk if this actually ever happens
            if features is not None and fontElem.get("features",None) != features:
                fontElem.set("features", features)
            if langSetting is not None and fontElem.get("lang",None) != langSetting:
                fontElem.set("lang", langSetting)
            swapNeeded = False
        if keep == False:
            fontElem.parent.remove(fontElem)
            if swapperFont == None:
                #for rare edge cases where the easiest solution is to just delete it and don't replace with anything
                continue
        elif font in lffUrls.keys():
            if len(fontElem) == 0:
                #aka if there isn't a url AND this is one of the rare ones that actually gets a url even though a new thing is also being added as supplement (currently this is just Annapurna SIL Nepal)
                linksMissing = _addLink(font, fontElem)
        if swapNeeded and swapperFont not in fontList.keys() and swapperFont not in newSwaps.keys():
            #newSwaps is here to make sure we don't replace two fonts with the same new font. 
            #For example, if Andika wasn't in the original collection of fonts (fontList), but then font #1 swapped to andika, and font #2 would ALSO swap to andika, it instead just skips it since we already now have andika
            if langSetting is None and features is None:
                fontElem = ldml.ensure_path('special/sil:external-resources/sil:font[@name="{}"]'.format(swapperFont))[0]
            elif langSetting is not None and features is None:
                fontElem = ldml.ensure_path('special/sil:external-resources/sil:font[@name="{}"][@lang="{}"]'.format(swapperFont, langSetting))[0]
            elif langSetting is None and features is not None:
                fontElem = ldml.ensure_path('special/sil:external-resources/sil:font[@name="{}"][@features="{}"]'.format(swapperFont, features))[0]
            else:
                fontElem = ldml.ensure_path('special/sil:external-resources/sil:font[@name="{}"][@features="{}"][@lang="{}"]'.format(swapperFont, features, langSetting))[0]
            newSwaps[swapperFont]= fontElem
            font=swapperFont
        elif swapNeeded and (swapperFont in fontList.keys() or swapperFont in newSwaps.keys()):
            if swapperFont in fontList.keys():
                fontElem = fontList[swapperFont]
            else:
                fontElem = newSwaps[swapperFont]
            # if the font was already added by something else, but not the appropriate features, add the features to the previously added font
            if features is not None and fontElem.get("features",None) != features:
                fontElem.set("features", features)
            if langSetting is not None and fontElem.get("lang",None) != langSetting:
                fontElem.set("lang", langSetting)
            swapNeeded = False
        else:
            #if the font we would swap to was already in the file (fontList) or was added to the file from a previous swap in the loop (newSwaps), skip it
            continue
        if len(fontElem) == 0:
            #aka if there isn't a url
            linksMissing = _addLink(font, fontElem)

        if font in automaticDefaults:
            fontElem.set("types", "default")
        if font in stowaways.keys() and script not in stowaways[font]:
            fontElem.parent.remove(fontElem)
            if len(fontList.items()) == 1:
                #aka if you just deleted the only font in the list
                fonts = ldml.findall('special/sil:external-resources/')
                fonts = _handleNoFonts(fonts)
                if len(fonts) > 0 :
                    font = fonts[0].get("name")
                    fontElem = fonts[0]
                else:
                    fontMissing = True
        
        if len(fonts) > 0:
            if len(fontElem) == 0:
                #aka if there isn't a url
                linksMissing = _addLink(font, fontElem)
                if linksMissing:
                    if keep:
                        pass    # this is one of the ones that we leave without links for now
                    else:
                        #print("{} is missing a url, please update the dictionaries in dbl2ldml accordingly".format(font))
                        # this references fontswap 
                        urlMissing = True

    fontErrors = [fontMissing, urlMissing, linklessFonts]
    return fontErrors



def processOneProject(filename, outputPath, ducetDict, langCode, sldrPath=None):
    dblObj = newdbl.DBL()
    dblObj.open_project(filename)

    s = str(filename).find(langCode)
    dblfile = str(filename)[s:]

    runFiles.append(dblfile)
    
    newDblFile = False
    newSldrFile = False

    filenames = {}
    fileExtCounts = {'ldml': 0, 'lds': 0, 'ssf': 0}  # for troubleshooting purposes
    filesAll = []   # for troubleshooting purposes
    wrongLangLds = 0
    wrongLangLdml = 0
    wrongLangFile = []
    wrongEspFile = 0
    wrongEngFile = 0
    for n in dblObj.namelist():
        for ext in ('ldml', 'lds', 'ssf'):
            if n.endswith("."+ext):
                fileExtCounts[ext] += 1     # for troubleshooting purposes
                filesAll.append(n)      # for troubleshooting purposes
                if n in ['release/English.lds'] or n[-8:] in ["_en.ldml"]:
                    # skip english completely because it won't add anything to a collation anyway 
                    # might skip over font data tho hmm
                    wrongLangFile.append(n)
                    wrongEngFile += 1
                    if ext == 'lds':
                        wrongLangLds += 1
                    elif ext == 'ldml':
                        wrongLangLdml += 1                    
                    continue
                elif n in ["release/Spanish.lds"] or n[-8:] in ["_es.ldml"]:
                    # this should ideally be commented out later
                    # primarily useful during this first run where we are filtering out the majority language files
                    wrongLangFile.append(n)
                    wrongEspFile += 1
                    if ext == 'lds':
                        wrongLangLds += 1
                    elif ext == 'ldml':
                        wrongLangLdml += 1
                    # notably this does NOT skip these entirely yet. The code further down skips the ldml if it's in the wrong language
                    # Spanish.lds however does NOT get skipped over unless there is a relevant lds file in the proper locale
                    # which is prioritized when the lds file gets processed. 
                    # ideally there should be a system in place where Spanish.lds is only referenced if the letters in the exemplars match the Spanish alphabet
                    # and/or there isn't a collation in a relevant ldml file. 
                    # But that requires cross-file stuff and storing info to compare what we have and don't have
                    # Not impossible but more work and I want to make sure this alone works first
                if ext not in filenames.keys():
                    filenames[ext] = [n]
                else:
                    filenames[ext].append(n)

    hasMetaDataFile = 'metadata.xml' in dblObj.namelist()
    
    #print(str(langCode) + ": " + str(fileExtCounts) + " " + str(filesAll))    # for troubleshooting purposes, uncomment when needed

    mainChTextOrig = None
    auxChTextOrig = None
    indexChTextOrig = None
    punctChTextOrig = None

    exemplarinfo = {}
    ldml = None
    dblLdml = None
    ltagp = None
    ssf = None
    lang = None
    script = None
    knownVariant = False
    varRegion = None
    
    if dblfile in skipfilesmap:
        reason = skipfilesmap[dblfile]
        logging.info("Skipping {}: {}".format(dblfile, reason))
        return

    if dblfile in knownvarsmap:
        langCode = knownvarsmap[dblfile]
        knownVariant = True

    b = str(langCode).find('-')
    if b > 0:
        fileLang = str(langCode)[:b]
    else:
        fileLang = str(langCode)
        #preserves a reference to the tag the file was originally under before finding the true minimal tag to use in sldr
        
    a = str(langCode).rfind('-')
    if knownVariant and a > 0 and str(langCode)[a+1].isupper() and str(langCode)[a+2].isupper():
        ltag = langtag(langCode)
        varRegion = str(langCode)[a+1] + str(langCode)[a+2]
        #this is because some of the knownVariants distinguish between regions that langtags will consider one tag and will therefore remove the region under the "minimal" tag
    else:
        try:
            ltag = langtag(str(lookup(langCode).tag))
        except KeyError:
            iso639_3 = iso639_3_2(langCode)
            if iso639_3:
                ltag = langtag(str(lookup(iso639_3).tag))
                langCode = iso639_3
            else:
                ltag = langtag(langCode)

    r = str(ltag).find('-')
    if r > 0:
        lang = str(ltag)[:r]
    else: 
        lang = str(ltag)

    exemplars = Exemplars()
    exemplars.frequent = 0.0
    for t in dblObj.analyze_text():
        exemplars.process(t)
    exemplars.analyze()
    exemplars.normalize("NFC")
    if len(exemplars.script) > 0: 
        script = exemplars.script
        # for cases where the translation text is for a more specific locale than the "encompassed" one from the langCode, i.e. file might say "wsg" but the locale is actually "wsg_Telu"
        if not knownVariant:
            exemplarlangcode = lang + "-" + exemplars.script
            try:
                exemplartag = langtag(str(lookup(exemplarlangcode).tag))
            except KeyError: 
                exemplartag = langtag(exemplarlangcode)
            if exemplartag != ltag: 
                langCode = exemplartag

    if 'ldml' in filenames:
        for c in filenames['ldml']:
            with dblObj.project.open(c) as inf:
                tempDblLdml = Ldml(inf)
                templdmllang = tempDblLdml.root.find('.//identity/language')
                if templdmllang != None and templdmllang.get('type') != lang and templdmllang.get('type') != fileLang:
                    #skips ldml files that arent for the locale, such as leftover majority lang files
                    continue
                dblLdml = tempDblLdml
                ldmllang = templdmllang
                ldmlscript = dblLdml.root.find('.//identity/script')
                if ldmlscript != None and script != None and ldmlscript.get('type') != script:
                    #the script listed in the ldml file does not match the script of the dbl file. 
                    #changing the script to None so that this doesn't mess with the listed langtag
                    ldmlscript = None
                ldmlterritory = dblLdml.root.find('.//identity/territory')
                ldmlvar = dblLdml.root.find('.//identity/variant')
                ldmllangcode = None
                if ldmllang != None and not knownVariant: 
                    ldmllangcode = ldmllang.get('type')
                    if ldmlscript != None: 
                        ldmllangcode = ldmllangcode + "-" + ldmlscript.get('type')
                    elif script != None:
                        ldmllangcode = ldmllangcode + "-" + script  #for cases where the tag would change if the script isn't included and the ldml forgot to add it, it pulls in the script we already identified from the translation body text
                    if ldmlterritory != None: 
                        ldmllangcode = ldmllangcode + "-" + ldmlterritory.get('type')
                    if ldmlvar != None:
                        ldmllangcode = ldmllangcode + "-" + ldmlvar.get('type')
                # for cases where the ldml file is for a more specific locale than the "encompassed" one from the langCode
                if ldmllangcode != None:
                    try:
                        ldmltag = langtag(str(lookup(ldmllangcode).tag))
                        if (ldmllang.get('type') == lang or ldmllang.get('type') == fileLang) and ldmltag != ltag:       # checks against 'lang' to avoid cases when the only ldml file in the project isn't for the language at all
                            langCode = ldmltag
                    except KeyError: 
                        pass
                      
    if 'ssf' in filenames:
        with dblObj.project.open(filenames['ssf'][0]) as inf:
            #have this as simply grabbing the first one because I'm pretty sure ssf doesn't appear in dbl release directory anymore but I don't want it to break if it ever does
            ssf = Ssf()
            ssf.parseSsf(inf)
            ltagp = ssf.get_lang()
    elif dblLdml is not None:
        ssf = Ssf()
        ssf.fromLdml(dblLdml, langCode)
    if not knownVariant:
        try:
            ltag = langtag(str(lookup(ltagp or langCode).tag))
        except KeyError:
            ltag = langtag(ltagp or langCode)
    outfname = str(ltag).replace("-","_")+".xml"

    hasldml = False
    if sldrPath is not None:
        testpath = os.path.join(sldrPath, outfname[0], outfname)
        if os.path.exists(testpath):
            ldml = Ldml(testpath)
            identity = ldml.root.find(".//identity/special/sil:identity", {v:k for k,v in ldml.namespaces.items()})
            if identity is not None and identity.get("source", "") == "cldr":
                logging.debug("Skipping {} since in CLDR".format(filename))
                return
            hasldml = True
            genDate = ldml.root.find(".//identity/generation")
            version = ldml.root.find(".//identity/version")
            if version is not None:
                versionDate = version.get("number")
            else:
                versionDate = None
            if genDate is None and len(versionDate) < 7:
                # if it doesn't have a "generation" element or the version number isn't a date
                # it means the current ldml file wasn't generated via dbl
                # these should def be checked manually for any weirdness
                newDblFile = True
    if ldml is None:
        newDblFile = True
        newSldrFile = True
        ldml = dblLdml if dblLdml is not None else Ldml(None)
        ldml.use_draft = 'generated'
        ldml.default_draft = 'generated'
    chElems = ldml.root.findall('characters/exemplarCharacters')
    for chEl in chElems:
        exemType = chEl.get('type')
        exemplarinfo[chEl.get('type', '')] = (chEl.text, ldml.get_draft(chEl))

    ldml.uid = "dbl"  # generates alt="proposed-dbl"

    # Any <cr> element in <collation> is suspect. In fact, just go ahead and delete it.
    nosort = True
    sortColl = None
    collationNode = ldml.find('collations/collation[@type="standard"]')
    removeTheseCollations = []
    if ldml.find('collations') != None:
        for extraCollation in ldml.find('collations'):
            if extraCollation is not None and collationNode is not None:
                if extraCollation.get("type") != collationNode.get("type") and extraCollation.get("type") != None:
                    removeTheseCollations.append(extraCollation)
        if len(removeTheseCollations) > 0:
            for removeMe in removeTheseCollations:
                removeMe.parent.remove(removeMe)
    dblSimpleCollation = None
    collCrNode = None
    replaceColl=True
    collSimpleNode=None
    if dblLdml is not None:
        dblSimpleCollation = dblLdml.find('collations/collation[@type="standard"]/special/sil:simple')
        nosort = False
    if collationNode is not None:
        collCrNode = collationNode.find('cr')
        collSimpleNode = collationNode.find('special/{{{}}}simple'.format(ldml.silns))
        if collCrNode is not None and ldml.get_draft(collCrNode) >= gendraft:
            replaceColl = True
            if collSimpleNode is not None:
                if ldml.get_draft(collSimpleNode) < gendraft:
                    dblSimpleCollation = collSimpleNode
                collationNode.remove(collCrNode)
        else:
            replaceColl = False
    if langCode in dontaddcollation:
        replaceColl = False
    sortSpecString = ''
    prioritizeLdmlColl = True
    vs=None
    collMatchesScript=True
    ldsvs=None
    if replaceColl:
        if 'lds' in filenames and (collSimpleNode == None or ldml.get_draft(collSimpleNode) >= gendraft):
            for d in filenames['lds']:
                if d == "release/Spanish.lds":
                    if fileExtCounts['lds'] > 1 or dblSimpleCollation is not None:
                        #this will still grab the spanish one if there is no other lds file or if there is no collation in the dblldml file
                        continue
                with codecs.getreader("utf-8")(dblObj.project.open(d)) as inf:
                    (sortColl, alphabet) = processLds(ldml, inf, ducetDict, replaceColl, filename, script)
                if sortColl != None:
                    prioritizeLdmlColl = False
                    # breaks the moment there is a sortColl
                    # this is to avoid situations where there is more than one viable lds file
                    # not perfect solution but will prevent any potential redundancy weirdness
                    # currently only file that would use this is alp
                    ldsvs = re.sub(r"(\s*\n)+", "\n", sortColl.asSimple())
                    break
        if dblSimpleCollation is not None:
            vs = re.sub(r"(\s*\n)+", "\n", dblSimpleCollation.text)
            if prioritizeLdmlColl: 
                vstrings = [puamap(s) for s in vs.split("\n")]
                for v in vstrings:
                    if Script.getShortName(Script.getScript(v[0])) != script and Script.getShortName(Script.getScript(v[0])) not in ['Zyyy', 'Zinh']:
                        #simple collation has character that isn't in the script, nor is it punctuation nor diacritic
                        #alternatively it could be handy to have a loop that removes non-script matching characters
                        collMatchesScript = False
                        vs = None
                        break
                if collMatchesScript:
                    (sortColl, alphabet) = calcSortLdsData(ldml, filename, vstrings, ducetDict, script)
        elif collMatchesScript and ldsvs != None:
            #this allows for dblSimpleCollation to still be prioritized because it's more likely to actually be tailored, while still keeping the one from the lds file if the ldml one is for the wrong script
            vs = ldsvs
        if vs is not None:
            if len(vs) > 0:
                testvs = vs
                se = ldml.ensure_path('collations/collation[@type="standard"]/special/sil:simple')[0]
                se.text = puamap(vs)
        if sortColl is not None and len(sortColl) > 0:
            #thisColl = copy.deepcopy(sortColl)
            thisColl = sortColl.copy()    
            try:
                thisColl.minimise()
                # print(thisColl)     
            except Exception as errorname:
                logging.debug("Unable to MINIMIZE ldml collation for {}; let's see if collation still works: ".format(filename) + str(type(errorname).__name__) + " - " + str(errorname))
                pass
            try:
                sortSpecString = thisColl.asICU() 
                # this next bit until the end of the for loop is meant to bandaid-fix the fact that asICU escapes things using backslash instead of the single quotation marks that is needed for collation
                fixEscapeString = ""
                index = 0
                hold = False
                for s in sortSpecString:
                    if s == "\\" and index > 0:
                        if sortSpecString[index-1] == " " or sortSpecString[index-1] == "&":
                            s = "'"
                            hold = True
                    if s == "\n" and hold:
                        s = "'\n"
                        hold = False
                    elif s == " " and hold:
                        s = "' "
                        hold = False
                    if (index+1) == len(sortSpecString) and hold:
                        s += "'"
                    fixEscapeString += s
                    index += 1
                sortSpecString = fixEscapeString
                if len(sortSpecString):
                    collationElem = ldml.ensure_path('collations/collation[@type="standard"]')[0]
                    crElem = ldml.addnode(collationElem, 'cr', returnnew=True)
                    crElem.text = sortSpecString
                elif dblSimpleCollation != collSimpleNode:
                    removeSimple = ldml.find('collations/collation[@type="standard"]')
                    collNode = removeSimple.parent
                    collNode.remove(removeSimple)
                    if not len(collNode):
                        collNode.parent.remove(collNode)
            except Exception:
                #no debug message because this error will ALSO ping the setSortKeys error a few lines down
                pass
        elif not nosort:   # delete the collation
            # only delete collation if the original was a simple sort. If hand crafted, keep it.
            if collationNode is not None and collationNode.find('special/{{{}}}simple'.format(ldml.silns)) is not None:
                collNode = collationNode.parent
                collNode.remove(collationNode)
                if not len(collNode):
                    collNode.parent.remove(collNode)
    
    collationWorked = False
    if sortColl is not None:
        try:
            sortColl._setSortKeys()
            collationWorked = True
            exemplars.collator = sortColl
        except:
            logging.debug("Unable to create ldml collation for {}; something goes wring in '._setSortKeys' in 'collation.py' in sldrtools, collation being skipped for now".format(filename))
            pass

    for t in ('auxiliary', 'main', 'index', 'punctuation'):     # order is important
        if t == 'main':
            key = ''
            exemplars._main -= exemplars._auxiliary
        key = '' if t == 'main' else t
        xpath = 'characters/exemplarCharacters[@type="' + key + '"]'
        oldValue, oldDraft = exemplarinfo.get(key, ('', 6))
        newValue = puamap(getattr(exemplars, t))        # sort these perhaps in analyze?
        oldSets = UnicodeSets.parse(oldValue)
        oldSet = oldSets[0].asSet() if len(oldSets) else set()
        newSet = getattr(exemplars, "_"+t)
        chElem = None
        if oldValue == '':
            if newValue != "[]":
                # no previous value; mark new value generated
                chElem = ldml.ensure_path(xpath)[0]
        elif newSet != oldSet:
            if oldValue == "[]" or oldDraft >= gendraft:
                # Throw away old empty value
                chElem = ldml.ensure_path(xpath)[0]
                ldml.change_draft(chElem, "generated")
            elif t == 'auxiliary' and oldDraft < gendraft:
                setattr(exemplars, t, oldValue)
        if chElem is not None:
            chElem.text = newValue
            
    fonts =  ldml.root.findall('special/sil:external-resources/sil:font', {v:k for k,v in ldml.namespaces.items()})
    fontErrors = addMissingFontData(ldml, fonts, script)
        # fontErrors is a list containing [bool, bool, [list]] where item 0 is true if there is no font, item 1 is true if there is a font without a link, and item 2 contains a list of the fonts without links (or an empty list if item 1 is False)

    # Delete empty data from original LDML
    likelyEmptyAttrs = ['crossrefs', 'diacritics', 'footnotes', 'verseSegments', 'wordBreaks', 'wordFormingPunctuation']
    specialNode = ldml.find('characters/special')
    for attr in likelyEmptyAttrs:
        node = ldml.find('characters/special/sil:exemplarCharacters[@type="' + attr + '"]')
        if node is not None and (node.text == '' or node.text == '[]'):
            specialNode.remove(node)
    if specialNode is None:
        pass
    elif len(specialNode) == 0:  # do we still need this case?
        charNode = ldml.find('characters')
        charNode.remove(specialNode)

    if ssf is not None:
        ssf.process(ldml, filename)


    if hasMetaDataFile:
        with dblObj.project.open("metadata.xml") as inf:
            ltagp = processMetadata(ldml, inf)

    # if fileExtCounts['lds'] == wrongLangLds and fileExtCounts['ldml'] == wrongLangLdml and collationNode is not None:
    #     if wrongEspFile > 0: 
    #         logging.warning("{} contains no data-driven files specifically regarding this locale, only {}, which are in Spanish. Ensure collation data is correct in past generated files".format(dblfile, wrongLangFile))    
    #     if wrongEngFile > 0:
    #         #logging.warning("{} contains no data-driven files specifically regarding this locale, only {}, which are in English. This shouldn't impact anything directly but should be noted".format(dblfile, wrongLangFile))    
    #         pass
    # elif wrongEspFile > 0 and collationWorked == False and collationNode is not None:
    #     logging.warning("{} pulls its collation data from 'Spanish.lds', ensure collation data is correct in past generated files".format(dblfile, wrongLangFile))
    #     #print(filenames)   # if you want context for what the files that do work with the locale are
    # elif wrongEngFile > 0 and collationWorked == False:
    #     #logging.warning("{} has some files in the correct locale and some files {} with data from English. This shouldn't impact anything directly but should be noted".format(dblfile, wrongLangFile))
    #     pass

    dblObj.close_project()

    # Version
    if not hasldml:
        # only sort out identity if we are creating rather than editing sldr file
        dateNow = datetime.utcnow()
        genTimeValue = datetime.strftime(dateNow, "%Y%m%d.%H%M")
        ldml.remove_path("identity")
        versionNode = ldml.ensure_path('identity/version', draft="unconfirmed")[0]
        versionNode.set('number', genTimeValue)
        ldml.ensure_path('identity/language[@type="{}"]'.format(ltag.lang), draft="unconfirmed")
        # there may be a bug in here that doesn't add one of these fields where it should if the tag gets changed later in the process OR adds a tag that is redundant. not sure. 
        if ltag.script is not None:
            ldml.ensure_path('identity/script[@type="{}"]'.format(ltag.script), draft="unconfirmed")
        if ltag.region is not None:
            ldml.ensure_path('identity/territory[@type="{}"]'.format(ltag.region), draft="unconfirmed")
        if ltag.vars or ltag.ns:
            bits = []
            if ltag.vars is not None:
                bits.extend(ltag.vars)
            if ltag.ns:
                for k, v in sorted(ltag.ns.items()):
                    bits.extend([k] + v)
            ldml.ensure_path('identity/variant[@type="{}"]'.format("-".join(bits)), draft="unconfirmed")
        identity = ldml.ensure_path('identity/special/sil:identity', draft="generated")[0]
        try:
            ltagset = lookup(str(ltag))
            if ltagset.region is not None:
                if ltag.region is not None:
                    identity.set('defaultRegion', ltag.region)
                else:
                    identity.set('defaultRegion', ltagset.region)
            if ltagset.script is not None and ltag.script is None:
                identity.set('script', ltagset.script)
        except KeyError:
            pass

    outdir = os.path.join(outputPath, str(ltag)[0])
    if not os.path.exists(outdir):
        os.makedirs(outdir, exist_ok=True)
    ldmlOutputFilename = os.path.join(outdir, outfname)
    ldml.normalise()
    ldml.save_as(ldmlOutputFilename)
    logging.info("{} successfully processed under: {}".format(dblfile, outfname))

    # add info to the global dictionaries that will summarize anything needing manual review at the end of the import process
    if newDblFile == True:   # used to identify new DBL files for manual review
        if newSldrFile:
            newDblFiles[outfname] = dblfile  
        # else:
        #     newDblFiles[outfname] = [dblfile, "New DBL file with pre-existing SLDR file"]     # commented out because I realized that since existing sldr files don't get a generation code they will ping this every time      
    allGeneratedFiles[dblfile] = outfname   # used to identify any duplicates that overrode each other
    if fontErrors[0] or fontErrors[1]:
        fontIssues[outfname] = [dblfile]
        if fontErrors[0]:
            fontIssues[outfname].append("Font Missing")
            fontIssues[outfname].append("Add a default font for {} into the defaultFonts dictionary in the addMissingFontData function in dbl2ldml".format(script))
        if fontErrors[1]:
            fontIssues[outfname].append("Font URL Missing")
            fontIssues[outfname].append(fontErrors[2])

# end of processOneProject

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('outpath',help="Directory to store generated ldml files in if not -l")
    parser.add_argument('-d','--dblpath',help="Path to local zips of DBL")
    parser.add_argument('-s','--sldrpath',help="Path to SLDR root for testing for CLDR files not to process")
    parser.add_argument('-u','--update',action='store_true',help='Update .zip files in dblpath')
    parser.add_argument('-m','--map',help="paratext project to langtag map .json")
    parser.add_argument('-L','--lang',help='Only process given language')
    parser.add_argument('-j','--jobs',type=int,default=1,help="Number of parallel processes to run, 0 = default = number of processors")
    parser.add_argument('-z','--zipfile',help='Process a specific .zip file')
    parser.add_argument('--ldml',help='input LDML base file to directly process')
    parser.add_argument('--ssf',help='input SSF file to directly process')
    parser.add_argument('--lds',help='input LDS file to directly process')
    parser.add_argument('-S','--start',help='skip up to an including this langtag')
    parser.add_argument('-l','--loglevel',help='Set logging level')
    parser.add_argument('-D','--debug',action="store_true",help="Enable debug")
    parser.add_argument('-Z','--zdebug',default=0,type=int,help="bitfield: 1=don't download zips, 2=skip existing")

    args = parser.parse_args()

    import sys
    if args.loglevel:
        logging.basicConfig(stream=sys.stdout, level=args.loglevel.upper(),
                format="%(levelname)s:%(module)s %(message)s")
        
    (skipfilesmap, knownvarsmap) = newdbl.exceptions()

    def processfile(f, l, ducetDict):
        logging.info("Processing file: {}".format(f))
        try:
            processOneProject(f, args.outpath, ducetDict, l, sldrPath=args.sldrpath)
        except Exception as e:
            bt = traceback.format_exc(limit=5)
            logging.error("Error in {}, {}\nType: {} Args: {}".format(f, e, type(e), e.args))
            logging.error(bt)
            if args.debug:
                raise e
        if len(runFiles) == len(jobs):
            # this means the file that was just processed is the last on the list
            # therefore, time to spit out any summaries of the data that you want to have
            # this has to be here instead of at the end of the code bc if multiprocessing is used then it will not see the global variables
            if len(newDblFiles.items()) > 0:
                print("LDML files generated that don't currently exist in the SLDR, with their respective DBL project file:")
                for n in newDblFiles.items():
                    print("\t" + str(n))
            else:
                print("No new LDML files generated from this batch of DBL files")

            duplicates = {}
            uniques = []
            for g in allGeneratedFiles.keys():
                outputFile = allGeneratedFiles[g]
                if outputFile not in uniques:
                    uniques.append(outputFile)
                else:
                    for k in [k for k,v in allGeneratedFiles.items() if v == outputFile]:
                        duplicates[k] = allGeneratedFiles[k]
            
            if len(duplicates.items()) > 0: 
                print("Multiple DBL files saved to the same path, therefore overriding one or more of them. Please address these in the 'skipfilesmap' or 'knownvarsmap' dictionaries in newdbl.py:")
                for d in duplicates.items():
                    print("\t" + str(d))

            if len (fontIssues.items()) > 0: 
                print("Issues with fonts in some files listed below. Please update the dictionaries in dbl2ldml accordingly:")
                # this references fontswap in addMissingFontData()
                for f in fontIssues.items():
                    print("\t" + str(f))
            print("ALL DONE!")
        return True

    if args.jobs == 1:
        pool = None
    else:
        pool = multiprocessing.Pool(processes=args.jobs)
    if args.update:
        rdr = newdbl.DBLReader()
        rdr.download(args.dblpath, lang=args.lang, nozips=args.zdebug & 1, mapfile=args.map, pool=pool)

    if args.sldrpath is not None:
        ducetDict = ducet.readDucet()

        if args.zipfile is None:
            filelist = [os.path.join(args.dblpath, f) for f in os.listdir(args.dblpath) if f.endswith(".zip")]
        else:
            filelist = [args.zipfile]
        jobs = [j+(ducetDict, ) for j in sorted(newdbl.process_projects(filelist, args.lang))]
        if args.start:
            for i, j in enumerate(jobs):
                if j[1] == args.start:
                    jobs = jobs[i+1:]
                    break
        if (args.zdebug & 2) != 0:
            jobs = [j for j in jobs if not os.path.exists(os.path.join(args.outpath, j[1][0], j[1].replace("-","_")+".xml"))]
        if pool is None:
            [processfile(*j) for j in jobs]
        else:
            asyncres = pool.starmap_async(processfile, jobs).get()
    if False:
        # Just process one set of files that is already present.
        ssfFile = args.ssf
        ldsFile = args.lds
        outFile = args.outpath

        if ldmlFile:
            ldml = Ldml(args.ldml)
        else:
            ldml = Ldml(None)
        ldml.use_draft = 'generated'
        if args.ssf:
            processSsf(ldml, args.ssf)
        if args.lds:
            import collation
            processLds(ldml, args.lds, ducetDict)
        if args.outfile:
            outf = codecs.open(args.outfile, 'w', encoding="utf-8")
        else:
            outf = sys.stdout
        ldml.serialize_xml(outf.write)
